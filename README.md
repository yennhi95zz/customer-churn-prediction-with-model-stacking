# Enhancing Customer Churn Prediction with Continuous Experiment Tracking in Machine Learning

![Author](https://img.shields.io/badge/Author-Nhi%20Yen-brightgreen)
[![Medium](https://img.shields.io/badge/Medium-Follow%20Me-blue)](https://medium.com/@nhiyen)
[![GitHub](https://img.shields.io/badge/GitHub-Follow%20Me-lightgrey)](https://github.com/nhiyen95)
[![Kaggle](https://img.shields.io/badge/Kaggle-Follow%20Me-orange)](https://www.kaggle.com/nhiyen95)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect%20with%20Me-informational)](https://www.linkedin.com/in/nhiyen95/)

### Introduction

Welcome to the GitHub repository for the project **Enhancing Customer Churn Prediction with Continuous Experiment Tracking in Machine Learning.** This hands-on project focuses on predicting customer churn in a telecom service using a model stacking approach. We aim to provide you with insights into dealing with classification problems, handling imbalanced datasets, and utilizing model stacking to enhance predictive performance.

![image](https://github.com/yennhi95zz/customer-churn-prediction-with-model-stacking/assets/88694623/efcf31b8-169c-41e5-8fdf-a4072a0d8864)


### Dataset

You can use the [Telco Customer Churn](https://www.kaggle.com/nhiyen95/telco-customer-churn) dataset from Kaggle. This dataset contains information about telecom customers, including various features like contract type, monthly charges, and whether the customer churned or not.

### Objective

The primary goal of this project is to predict customer churn, i.e., whether a customer will leave the telecom service or not. We achieve this by employing a model stacking approach, which involves training multiple machine learning models and combining their predictions using another model.

### Project Steps

Follow these steps to understand and reproduce the project:

1. Import Libraries: Import necessary libraries and initialize Comet ML.
2. Load and Explore Data: Load dataset and perform exploratory data analysis (EDA).
3. Preprocessing: Preprocess data by encoding and scaling features.
4. Model Training: Train multiple machine learning models, including Logistic Regression, Random Forest, Gradient Boosting, and Support Vector Machine.
5. Hyperparameter Tuning: Use Optuna to optimize hyperparameters for the models.
6. Ensemble Modeling: Create a stacking ensemble of models for improved predictions.
7. Optimization Results: Display the best hyperparameters and accuracy.
8. End Experiment: Conclude the Comet ML experiment.

This project will give you insights into dealing with classification problems, handling imbalanced datasets (if applicable), and utilizing model stacking to enhance predictive performance.

## References

- [GitHub Report](https://github.com/yennhi95zz/customer-churn-prediction-with-model-stacking)
- [Kaggle Project](https://www.kaggle.com/nhiyen/customer-churn-prediction-with-model-stacking)
- [Medium Article](https://medium.com/@yennhi95zz/a-hands-on-project-enhancing-customer-churn-prediction-with-continuous-experiment-tracking-in-77aeaff242f7)

Your support by giving this project a ⭐ is greatly appreciated and helps spread knowledge to others. If you found the article interesting, feel free to comment with your insights and share it with your network.

☕ [Buy Me a Coffee](https://paypal.me/yennhi95zz) to support my work!

Thank you for your interest in this project, and I hope you find it informative and valuable for your machine learning journey.

